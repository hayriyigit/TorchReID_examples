{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchreid\n",
    "from ultralytics import YOLO\n",
    "from ultralytics.trackers.utils.matching import linear_assignment\n",
    "import cv2\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: shufflenet_v2_x2_0\n",
      "- params: 5,344,996\n",
      "- flops: 379,562,752\n"
     ]
    }
   ],
   "source": [
    "extractor = torchreid.utils.FeatureExtractor(\n",
    "    model_name='shufflenet_v2_x2_0',\n",
    "    device='cuda'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLOv9c summary (fused): 384 layers, 25,380,928 parameters, 0 gradients, 102.7 GFLOPs\n"
     ]
    }
   ],
   "source": [
    "model = YOLO(\"yolov9c.pt\", task=\"detection\")\n",
    "model.fuse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 4 bottles, 1 tv, 2 mouses, 1 keyboard, 13.8ms\n",
      "1: 384x640 4 bottles, 1 tv, 1 mouse, 1 keyboard, 13.8ms\n",
      "Speed: 1.0ms preprocess, 13.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "im1 = cv2.imread(\"images/c1.jpeg\")\n",
    "# im1 = cv2.cvtColor(im1, cv2.COLOR_BGR2RGB)\n",
    "im1 = cv2.resize(im1, (1280, 720))\n",
    "\n",
    "im2 = cv2.imread(\"images/c2.jpeg\")\n",
    "# im2 = cv2.cvtColor(im2, cv2.COLOR_BGR2RGB)\n",
    "im2 = cv2.resize(im2, (1280, 720))\n",
    "\n",
    "anno = model([im1, im2], device=0)\n",
    "\n",
    "preds1 = anno[0].boxes.xyxy.cpu().numpy()\n",
    "preds2 = anno[1].boxes.xyxy.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 475 ms, sys: 9 Î¼s, total: 475 ms\n",
      "Wall time: 100 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cam1_features = []\n",
    "cam2_features = []\n",
    "for pred in preds1:\n",
    "    x1, y1, x2, y2 = np.intp(pred)\n",
    "    crop = im1[y1:y2, x1:x2, :]\n",
    "\n",
    "    feat = extractor(crop)[0].cpu().numpy()\n",
    "    feat = feat / np.linalg.norm(feat)\n",
    "    cam1_features.append(feat)\n",
    "\n",
    "for pred in preds2:\n",
    "    x1, y1, x2, y2 = np.intp(pred)\n",
    "    crop = im1[y1:y2, x1:x2, :]\n",
    "\n",
    "    feat = extractor(crop)[0].cpu().numpy()\n",
    "    feat = feat / np.linalg.norm(feat)\n",
    "    cam2_features.append(feat)\n",
    "    \n",
    "cam1_features = np.array(cam1_features)\n",
    "cam2_features = np.array(cam2_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_matrix = cam1_features @ cam2_features.T\n",
    "matched_indices, _, _ = linear_assignment(-sim_matrix, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, match in enumerate(matched_indices):\n",
    "    if sim_matrix[match[0], match[1]] < 0.1:\n",
    "        continue\n",
    "    else:\n",
    "        # Draw bounding boxes\n",
    "        x1, y1, x2, y2= np.intp(preds1[match[0]])\n",
    "        cv2.rectangle(im1, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        cv2.putText(\n",
    "            im1,\n",
    "            f\"{idx}\",\n",
    "            (x1, y1),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            1,\n",
    "            (0, 255, 0),\n",
    "            2,\n",
    "        )\n",
    "        x1, y1, x2, y2= np.intp(preds2[match[1]])\n",
    "        cv2.rectangle(im2, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        cv2.putText(im2, f\"{idx}\", (x1, y1), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow(\"im1\", im1)\n",
    "cv2.imshow(\"im2\", im2)\n",
    "\n",
    "cv2.waitKey()\n",
    "\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['resnet18', 'resnet34', 'resnet50', 'resnet101', 'resnet152', 'resnext50_32x4d', 'resnext101_32x8d', 'resnet50_fc512', 'se_resnet50', 'se_resnet50_fc512', 'se_resnet101', 'se_resnext50_32x4d', 'se_resnext101_32x4d', 'densenet121', 'densenet169', 'densenet201', 'densenet161', 'densenet121_fc512', 'inceptionresnetv2', 'inceptionv4', 'xception', 'resnet50_ibn_a', 'resnet50_ibn_b', 'nasnsetmobile', 'mobilenetv2_x1_0', 'mobilenetv2_x1_4', 'shufflenet', 'squeezenet1_0', 'squeezenet1_0_fc512', 'squeezenet1_1', 'shufflenet_v2_x0_5', 'shufflenet_v2_x1_0', 'shufflenet_v2_x1_5', 'shufflenet_v2_x2_0', 'mudeep', 'resnet50mid', 'hacnn', 'pcb_p6', 'pcb_p4', 'mlfn', 'osnet_x1_0', 'osnet_x0_75', 'osnet_x0_5', 'osnet_x0_25', 'osnet_ibn_x1_0', 'osnet_ain_x1_0', 'osnet_ain_x0_75', 'osnet_ain_x0_5', 'osnet_ain_x0_25']\n"
     ]
    }
   ],
   "source": [
    "torchreid.models.show_avai_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
